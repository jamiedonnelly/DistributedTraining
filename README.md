Collection of scripts for different implementations of parallel/distributed training in PyTorch, and using SLURM scheduler for coordination and resource allocation.
